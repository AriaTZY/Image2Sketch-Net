# Image to Sketch Network

## 1. Environment

- pytorch == 1.7.1 (>1.x.x should all be fine)
- torchvision == 0.8.2
- tensorflow  == 1.14.1 (1.12.0 <= version <=1.15.0)
- [opencv](https://opencv.org/) == 3.4.2



## 2. Test the full framework

### Framework Structure:

My framework consists of multiple steps:

**natural image  ---->  Mask Net  ---->  Sketch Net  ----->  Virtual Sketching ----> Sketch (stroke format)**

![pipeline-proposal](./docs/pipeline-proposal.jpg)



### Requirement of Test Images 

The network is mainly composed of "Mask Net" and "Sketch Net", among which Mask Net is an auxiliary function, so please keep the background of the input picture as simple as possible, and put the object you want to sketchlize in the middle of the image when you need to use MaskNet. Otherwise the network will not have any idea of which object should be selected.

![input_quality](./docs/input_quality.jpg)

Of course, if you decide not to use Mask Net, you can input any image you want to the network.



### Run it!

Here is the code you should type to test this framework:

``` shell
python pipeline.py --datapath 'Your/datapath/of/test/images/' --outpath 'The/folder/of/output/' --cuda True --version 2 --maskmode 'soft' --softctr 0.5 --resolution 2 --background True --vec_switch False --merge False --gamma True
```

==> **datapath:** it can be the folder or image (end with '.png' or '.jpg') path. When input folder path, make sure *you add '/' at last, or it will be an error, i.e., 'TestDataset/images/' instead of  'TestDataset/images'*

==> **version:** Mask Net version, 1 or 2, but 2 performs better

==> **maskmode:** how to binarize the generated mask, 'soft' is recommended. you also can use 'hard'

==> **softctr:** the threshold of soft binarization. Smaller value can expose more foreground object part, [0, 1]  float value.

==> **resolution:** The input image size. You can change this regardless your real input image size, but do not exceeded your input size is recommended. except 0 means 600,  other number means: N*424. e.g., 2 means the input image size will be resized to 2 * 424 = 828

==> **background:** whether to use Mask Net.

==> **vec_switch:** whether to use Virtual Sketching to convert to stroke format

==> **merge:** whether to use positive+negative and merge strategy

==> **gamma:** pre-process to enhance contrast



Or... If you want some simpler version:

``` shell
python pipeline.py --datapath 'Your/datapath/of/test/images' --outpath 'The/folder/of/output' --cuda True
```



Or... if you want just run using default without any change:

```shell
python pipeline.py --cuda True
```

* The default input is in folder "input/iphone/"

* The output folder will be in "output/Test888/"

So, basically, you can drop your pics into default input folder and harvest them in default output folder.



## 3. Quick Test

The core of my project is the **Sketch Net**, so if you just want to only test the performance/capacity of the Sketch Net, you can use this quick test.

Here doesn't provide shell *cmd* line, but you can run it in your IDE

*If you want to change the default input images as yours, please find here and change the "datapath" to yours*

```python
if args.mask:
    data_path = '../Dataset/masked_iphone/'
else:
    data_path = '../Dataset/iphone/'
```



## 4. Separate Test

* Separately running the "natural image --> raster form sketch" and "virtual sketching" processes
* Who can ignore this: 1) people don't want to generate stroke form sketch; 2) Who is very rich for super powerful GPU; 3) you don't need to run it on GPU.

I understand some computer is not very powerful (for example, MINE ğŸ˜Ÿ), while running those three network **on CUDA** in one-shot is impossible. So you have to separate to run them to get stroke form sketches.

My computer: 8GB GPU, when run resolution = 2 (i.e., 848x848 input image), the memory was excceeded.

---

First, Let's run **Mask Net** and **Sketch Net** to get raster form sketches

```shell
python pipeline.py --datapath 'Your/datapath/of/test/images/' --outpath 'The/folder/of/output/' --cuda True --vec_switch False
```

Then the you will find there are two folder under your output path

* gallery
* sketch: please do not delete this !

â€‹     

Second, run **"Virtual Sketching":**

```shell
python virtualSketchingBatch.py --cuda True --outpath 'The/path/you/want/to/out' --inpath 'sketch/path/as/generated/above' --sz 800
```

* sz: input resized size. Larger, more detailed output stroke sketch!

Done! 



## 5. Result

### 5.1 Full Framework Results

Here is the result of full pipeline, including Mask Net, Sketch Net, gamma enhancement (I didn't put the stroke form sketch here because it looks similar to the raster form)

---

**left:** natural images    

**Middle:** foregrounds cropped by Mask Net    

**Right:** generated Sketches by Sketch Net

![0dogs](./docs/0dogs.png)

![IMG_0036](./docs/IMG_0036.png)

![IMG_0827](./docs/IMG_0827.png)

![IMG_1061](./docs/IMG_1061.png)



### 5.2 Sketch Net Only Results:

Here is the results generated from the inputs which I manually crop the foreground objects. So it can eliminate the effect of Mask Net performance.

---

**Note:** Both the left and the right images represent the output sketch, they are identical. This is a compromise with the result display function.

![0](./docs/0.png)
![1](./docs/1.png)
![4](./docs/4.png)
![9](./docs/9.png)
![15](./docs/15.png)

# Appendix

## A1. File Structure Explanation

##### Sketch Net Part

* **dataloader_skt:** The data-loader for Sketch Net, including data augmentation function
* **network:** The architecture of Sketch Net, resemble to pix2pix (cGAN) network
* **test:** Solo test for Sketch Net
* **tools:** Some functions to visualize/save images/process mask image

##### Mask Net Part

* **MaskNet/dataloader_mask:** Dataloader for mask net
* **MaskNet/DataGeneration:** Generate training dataset from COCO, it followed by class
* **MaskNet/MaskNetwork:** Shallow Mask Net, output mask size is 28 x 28
* **MaskNet/MaskNetwork2** Deepper Mask Net, output mask size is 56 x 56
* **train_mask, test_mask:** similar to above

**Other Folders**

* **checkpoints:** The trained models, '.pth' format files
* **docs:** Some pics used in this README file
* **logs:** Some output images saved during training progress
* **MaskDataSet:** The dataset for training the Mask Net, generated from COCO dataset
* **output:** The output images of my network
* **Tensorboard:** tensorboard log files



## A2. Checkpoints Note:

the file under

* **MasNet_v2_notcentre:**The network trained on not centered mask image data. trained on 24/07/2021
* **MaskNet_v2_stage1**: The model trained on centered mask data and saved on halfway, seems not overfitting
* **MaskNet_v2_stage2:** The model saved after the full training, seems to have a serious overfitting
* **deep_GAN_L1:** loss function: balanced BCE, lr: 0.001, new vgg discriminator



## A3. 'Soft' and 'hard' binarization

High resolution with "soft" mask mode (with 824x824 input)

![000010](./docs/000010.png)

low resolution with "hard" mask mode, you can see the jagged boundary (with 424x424 input)

![000010_hard](./docs/000010_hard.png)



## A4. Compare with Canny Edge Detection

Canny detection requires manual adjustment of two threshold parameters to determine the complexity of the edge and the process is troublesome. What's more important is that canny cannot understand the image in a high and abstract level, so the edges are easily discontinuous, and the position of the lines will be the same as the original image.

![canny_compare](./docs/canny_compare.jpg)

## 2021.8.2 update log

1ï¼‰åœ¨â€œpipeline.pyâ€æ–‡ä»¶ä¸­å¢åŠ äº†backgroundçš„é€‰é¡¹ï¼Œç”¨äºæ˜¯å¦é€‰æ‹©è¿›è¡ŒMask Netæ“ä½œ

2ï¼‰åœ¨pipeline.pyä¸­åŸå…ˆç¬¬96è¡Œå·¦å³çš„ä½ç½®æœ‰ä¸€ä¸ªResizeçš„æ­¥éª¤ï¼Œä½†ä¹‹å‰ä¸€ç›´ç”¨çš„æ˜¯242ï¼ˆMask Netè¦æ±‚å°ºå¯¸ï¼‰ï¼Œæ‰€ä»¥å…¶å®å½“å†Resizeæˆä¸åŒå¤§å°è¾“å…¥Sketch Netçš„æ—¶å€™ï¼Œå·²ç»æ˜¯æåº¦æŸå¤±ä¿¡æ¯çš„äº†ã€‚æ‰€ä»¥ä¹‹å‰å¤§å°ºå¯¸è¡¨ç°å·®å°±æ˜¯å› ä¸ºè¿™ä¸ªã€‚*æˆ‘æ›´æ–°äº†è¿™ä¸ªé”™è¯¯ä»¥åï¼Œå…¶å®é«˜åˆ†è¾¨ç‡å›¾åƒå·²ç»è¡¨ç°å¾ˆå¥½äº†ï¼Œä½†æ˜¯æˆ‘è¿˜ç»§ç»­è®­ç»ƒ600å°ºå¯¸ç‰ˆæœ¬çš„ Sketch Netã€‚*ï¼ˆä¸Šï¼šæœ‰é”™è¯¯çš„ï¼Œä¸‹ï¼šæ›´æ–°é”™è¯¯åçš„ï¼‰

<img src="/docs/before_the_mistake.png" alt="before_the_mistake" style="zoom:50%;" />

<img src="/docs/after_correct_mistake.png" alt="after_correct_mistake" style="zoom:40%;" />

3ï¼‰è®­ç»ƒäº†600x600ç‰ˆæœ¬çš„Sketch Netï¼Œå…¶ä¸­ä¿®æ”¹äº†Dç½‘ç»œçš„lossï¼Œå§‹å¾—å…¶ä¸å†å¯¹å‡ºå…¥å›¾åƒæœ‰è¦æ±‚ï¼Œæœ€ç»ˆå¯ä»¥æ˜¯ä»¥1x1æˆ–æ˜¯2x2ç”šè‡³æ›´å¤§çš„ç‰¹å¾å›¾ä¸ºç½‘ç»œè¾“å‡ºï¼Œåªéœ€è¦æ›´æ”¹ä¸€ä¸‹targetï¼Œä½¿ç”¨expand_as å˜æˆåŒæ ·å¤§å°å³å¯ã€‚

4ï¼‰åœ¨æ›´æ–°Dç½‘ç»œçš„æ—¶å€™ï¼Œå¼€å§‹å¿˜è®°ç»™Gç½‘ç»œçš„è¾“å‡ºdetach()äº†ï¼Œä½†æ„Ÿè§‰åŠ äº†ä»¥ååˆæœŸä¹Ÿæ²¡æœ‰å½±å“å¾ˆå¤šï¼Œå¯ä»¥ç»§ç»­æ¢ç´¢ä¸€ä¸‹ä¹‹åã€‚

5ï¼‰æˆ‘å‘ç°å°†å›¾ç‰‡è°ƒé»‘ä¹‹åå†è¿‡ç½‘ç»œæ•ˆæœä¼šå¥½æŒºå¤šï¼ˆæš‚æ—¶ç”¨çš„PSï¼‰

## 2021.8.3 update log

### é’ˆå¯¹æ”¹å–„å°åŠ¨ç‰©ç­‰ç»†èŠ‚ä¸è¶³é—®é¢˜

 1ï¼‰å°è¯•ä»è®­ç»ƒæ•°æ®é›†å…¥æ‰‹ï¼Œå¯¹è®­ç»ƒé›†ç”Ÿæˆçš„è¿‡ç¨‹ä¸­ï¼šï¼ˆUCL_Generation/no_mask_pipeline.pyæ–‡ä»¶ä¸­ï¼‰

* åœ¨RGBåŸå›¾ä¸ŠåŠ å…¥ç›´æ–¹å›¾å‡è¡¡ï¼Œæ•ˆæœä¸æ˜æ˜¾
* åœ¨RGBåŸå›¾ä¸Šè°ƒä½äº®åº¦ï¼Œæ•ˆæœå‡ ä¹æ²¡æœ‰
* åœ¨pencilå›¾ï¼ˆå³å›¾åƒå¤„ç†ä¹‹åçš„å›¾ï¼‰ä¸ŠæŠ‘åˆ¶å™ªå£°ååšç›´æ–¹å›¾å‡è¡¡ï¼Œæ•ˆæœå¾ˆæ˜æ˜¾ï¼Œä½†åŒæ—¶å¯¹å™ªå£°çš„æ”¾å¤§ä¹Ÿä¸å«ç³Šï¼ŒåŸºæœ¬ä¸Šä¸èƒ½ç”¨ï¼Œå¤„ç†äººåƒå¦‚æ­¤ï¼Œå¤„ç†äº¤é€šå·¥å…·ç­‰å¯æƒ³è€ŒçŸ¥ã€‚
* ç»“è®ºï¼šè¿˜æ˜¯ä¿ç•™åŸæœ‰çš„gammaå˜æ¢æœ€æœ‰æ•ˆ

2ï¼‰åœ¨æµ‹è¯•æ—¶åšå‡ºæ”¹å˜ï¼Œå³æ”¹å˜è¾“å…¥è¿›ç½‘ç»œçš„å›¾ç‰‡ï¼šï¼ˆåœ¨MySketch/pipeline.pyæ–‡ä»¶ä¸­ç›´æ¥ä¿®æ”¹çš„ï¼‰

* åŸæ¥è¾“å…¥å›¾ç‰‡éƒ½æ²¡æœ‰åšgammaå˜æ¢ï¼Œè¿™æ˜¯æœ€å¤§çš„é—®é¢˜ä¹‹ä¸€ï¼Œè¡¥ä¸Šäº†æ•ˆæœæœ‰æ‰€èµ·è‰²ï¼ä½†å¯¹äºç¾Šç­‰è¿‡åº¦æ›å…‰çš„å›¾è¿˜æ˜¯ç»†èŠ‚ä¸æ„æ˜æ˜¾ã€‚ï¼ˆ**è®°å¾—åªç»™Sketch Netåšgammaå˜æ¢ï¼ŒMask Netä¸éœ€è¦ï¼Œå¦åˆ™maskå°†ä¸å‡†ç¡®**ï¼‰
* ä½¿ç”¨â€œæ­£ç‰‡+è´Ÿç‰‡â€å¤„ç†åå†ä½¿ç”¨bitwise_and()æ–¹æ³•ï¼Œç»†èŠ‚å…¨éƒ¨åˆ°ä½ï¼Œä½†ç¼ºç‚¹å°±æ˜¯ä¼šå‡ºç°â€œåŒè½®å»“â€ï¼å¤„ç†ç¾Šè¿™äº›å¾ˆå¥½ï¼Œä½†æ˜¯å¤„ç†äº¤é€šå·¥å…·å°±è¦æ–Ÿé…Œä½¿ç”¨ï¼Œå› ä¸ºä¼šå‡ºç°å¾ˆå¤šåŒçº¿æ¡ã€‚æ‰€ä»¥æˆ‘çš„æƒ³æ³•æ˜¯è®©ç”¨æˆ·è‡ªå·±é€‰æ‹©å¤„ç†æ–¹æ¡ˆã€‚

3ï¼‰æµ‹è¯•æ–°è®­ç»ƒå‡ºæ¥çš„600x600ç½‘ç»œ

* æœ‰å¯èƒ½æ˜¯è®­ç»ƒæ¬¡æ•°å¤ªå¤šï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯Gä¸Dçš„lossä¸å‡è¡¡çš„åŸå› ï¼ˆDçš„lossæœ€åéå¸¸å°å·²ç»æ˜¯0ï¼Œè€ŒGçš„lossè¿˜ç»´æŒä¸€å®šæ°´å¹³ï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯ç”±äºæ–°æ”¹å˜äº†Dçš„loss functionï¼Œ**è¿™ä¸ªæ–°è®­ç»ƒå‡ºçš„ç½‘ç»œä¼šå¾ˆå€¾å‘äºç”»å‡ºç‰©ä½“å¾ˆé»‘æš—çš„éƒ¨åˆ†ï¼Œå³æ¶‚è‰²ï¼Œå‡ºæ¥çš„æ•ˆæœå°±å’Œä½¿ç”¨åŸæ¥è®­ç»ƒçš„ç½‘ç»œä»¥424xçš„è¾“å…¥å·®ä¸å¤šï¼Œè™½ç„¶å›¾ç‰‡å°ºå¯¸å¢åŠ ï¼Œä½†åˆ†è¾¨ç‡å¹¶æ²¡æœ‰æé«˜ã€‚**
* å½“æˆ‘ä½¿ç”¨äºŒå€äºå®ƒçš„è¾“å…¥ï¼ˆ1200xï¼‰è¿›è¡Œæµ‹è¯•æ—¶ï¼Œæµ‹è¯•ç»“æœå’Œä¹‹å‰å·®ä¸å¤šï¼Œçº¿æ¡å˜ç»†ï¼Œç»†èŠ‚å¢åŠ ã€‚ï¼ˆä½†æ˜¯è¿˜æ˜¯æœ‰ç‚¹å€¾å‘äºç”»å‡ºæ¶‚è‰²çš„åœ°æ–¹ï¼‰ã€‚æ€»ä¹‹æ›´å€¾å‘äºä½¿ç”¨ä¹‹å‰çš„
* ç»“è®ºï¼šæ²¡æœ‰å¿…è¦åœ¨è®­ç»ƒæ—¶è°ƒèŠ‚è¾“å…¥å›¾ç‰‡å°ºå¯¸ï¼Œé‡ç‚¹åœ¨æµ‹è¯•æ—¶æ”¹å˜ä¼šè¾¾åˆ°ä¸åŒçš„ç»†èŠ‚æ•ˆæœã€‚

## 2021.8.4 update log

#### è¾“å‡ºè¯´æ˜

åœ¨ â€œoutput/Pipeline_and_vectorizeâ€ æ–‡ä»¶å¤¹ä¸­æœ‰ä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼Œåˆ†åˆ«æ˜¯åœ¨ 848x åˆ†è¾¨ç‡è¾“å…¥è¿›å…¥Sketch Netå’Œ 424xåˆ†è¾¨ç‡è¾“å…¥çš„åŒºåˆ«ã€‚å¯¹äºVectorizeçš„è¾“å…¥å›¾ç‰‡å°ºå¯¸ï¼Œéƒ½ä¸º600xï¼Œä½¿ç”¨çš„æ˜¯ç¬¬ä¸€ç‰ˆè®­ç»ƒç½‘ç»œæƒé‡ã€‚æµ‹è¯•å›¾åƒéƒ½æ˜¯iphoneæ‰‹æœºæ‹æ‘„å›¾ç‰‡ã€‚

åœ¨æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­åˆæœ‰è¿™ä¹ˆå‡ ä¸ªåˆ†ç±»ï¼š

* galleryï¼šä¸‰è”æ¨ªæ’æ˜¾ç¤ºï¼ˆä½†æ²¡æœ‰æœ€åçš„vectoræ˜¾ç¤ºï¼‰
* gif: vectorizeåçš„gifåŠ¨å›¾
* seq_dataï¼šéƒ½ä¸ºnpzæ ¼å¼æ•°æ®ï¼Œå­˜å‚¨ç¬”ç”»çš„åŸå§‹æ–‡ä»¶
* sketchï¼šå›¾ç‰‡ç»è¿‡Sketchç½‘ç»œä¹‹åçš„åŸå§‹è¾“å‡º
* vectorï¼šç›¸å½“äºæŠŠgifçš„æœ€åä¸€å¸§ä¿å­˜ä¸‹æ¥äº†ï¼Œé™æ€å›¾
